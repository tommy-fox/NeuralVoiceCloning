{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdf6847",
   "metadata": {},
   "source": [
    "### Install packages and download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ad1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#pip install SoundFile torchaudio munch torch pydub pyyaml librosa nltk matplotlib accelerate transformers phonemizer einops einops-exts tqdm typing-extensions\n",
    "#!sudo apt-get install espeak-ng\n",
    "#pip install git+https://github.com/resemble-ai/monotonic_align.git\n",
    "# git-lfs clone https://huggingface.co/yl4579/StyleTTS2-LibriTTS\n",
    "# mv StyleTTS2-LibriTTS/Models ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a5805",
   "metadata": {},
   "source": [
    "### Dataset -> StyleTTS2/Data (LJSpeech, 200 samples, ~15 minutes of data)\n",
    "You can definitely do it with fewer samples. This is just a proof of concept with 200 smaples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e5e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ibrazebra/Developer/Github/NeuralVoiceCloning/StyleTTS2/colab\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb63d17",
   "metadata": {},
   "source": [
    "### Navigate to the /NeuralVoiceCloning directory prior to executing the train_finetune.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(\"..\") # to go back a level\n",
    "os.listdir() # to list files in the current directory\n",
    "#os.chdir('NeuralVoiceCloning/') # to go into the directory (only if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9043901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ibrazebra/Developer/Github/NeuralVoiceCloning'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127b886",
   "metadata": {},
   "source": [
    "### Change the finetuning config\n",
    "\n",
    "Depending on the GPU you got, you may want to change the bacth size, max audio length, epochs and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a435667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = \"../configs/config_ft.yml\"\n",
    "\n",
    "# import yaml\n",
    "# config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['data_params']['root_path'] = \"Data/wavs\"\n",
    "\n",
    "# config['batch_size'] = 2 # not enough RAM\n",
    "# config['max_len'] = 100 # not enough RAM\n",
    "# config['loss_params']['joint_epoch'] = 110 # we do not do SLM adversarial training due to not enough RAM\n",
    "\n",
    "# with open(config_path, 'w') as outfile:\n",
    "#   yaml.dump(config, outfile, default_flow_style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50e773",
   "metadata": {},
   "source": [
    "### CHECK IF AUDIO FILES ARE IN VALID FORMAT....\n",
    "- samplerate: 24000 Hz ✅\n",
    "- channels: 1 ✅ (mono)\n",
    "- subtype: Signed 16 bit PCM ✅ (16-bit PCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a126ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/wavs/LJ001-0110.wav\n",
      "samplerate: 24000 Hz\n",
      "channels: 1\n",
      "duration: 5.870 s\n",
      "format: WAV (Microsoft) [WAV]\n",
      "subtype: Signed 16 bit PCM [PCM_16]\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "info = sf.info(\"Data/wavs/LJ001-0110.wav\")\n",
    "print(info)\n",
    "# Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257068a",
   "metadata": {},
   "source": [
    "### Start finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8821de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/ibrazebra/Developer/Github/NeuralVoiceCloning/StyleTTS2/train_finetune.py\", line 711, in <module>\n",
      "    main()\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/click/core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/click/core.py\", line 1082, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/click/core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/click/core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/Users/ibrazebra/Developer/Github/NeuralVoiceCloning/StyleTTS2/train_finetune.py\", line 133, in main\n",
      "    _ = [model[key].to(device) for key in model]\n",
      "  File \"/Users/ibrazebra/Developer/Github/NeuralVoiceCloning/StyleTTS2/train_finetune.py\", line 133, in <listcomp>\n",
      "    _ = [model[key].to(device) for key in model]\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3162, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "  File \"/opt/miniconda3/envs/tts-troopers/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 310, in _lazy_init\n",
      "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "AssertionError: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "!python -m StyleTTS2.train_finetune --config_path StyleTTS2/configs/config_ft.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df2112",
   "metadata": {},
   "source": [
    "### Test the model quality\n",
    "Note that this mainly serves as a proof of concept due to RAM limitation of free Colab instances. A lot of settings are suboptimal. In the future when DDP works for train_second.py, we will also add mixed precision finetuning to save time and RAM. You can also add SLM adversarial training run if you have paid Colab services (such as A100 with 40G of RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
    "\n",
    "config = yaml.safe_load(open(\"Models/LJSpeech/config_ft.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config['model_params'])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(\"Models/LJSpeech/\") if f.endswith('.pth')]\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_whole = torch.load(\"Models/LJSpeech/\" + sorted_files[-1], map_location='cpu')\n",
    "params = params_whole['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.diffusion import DiffusionSampler, ADPM2Sampler, KarrasSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f585a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d08dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, ref_s, alpha = 0.3, beta = 0.7, diffusion_steps=5, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = word_tokenize(ps[0])\n",
    "    ps = ' '.join(ps)\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(noise = torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "                                          embedding=bert_dur,\n",
    "                                          embedding_scale=embedding_scale,\n",
    "                                            features=ref_s, # reference from the same speaker as the embedding\n",
    "                                             num_steps=diffusion_steps).squeeze(1)\n",
    "\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha)  * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta)  * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en,\n",
    "                                         s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr,\n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "\n",
    "    return out.squeeze().cpu().numpy()[..., :-50] # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6f6d4",
   "metadata": {},
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Maltby and Company would issue warrants on them deliverable to the importer, and the goods were then passed to be stored in neighboring warehouses.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a83b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random reference in the training set, note that it doesn't matter which one you use\n",
    "path = \"Data/wavs/LJ001-0110.wav\"\n",
    "# this style vector ref_s can be saved as a parameter together with the model weights\n",
    "ref_s = compute_style(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0681bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wav = inference(text, ref_s, alpha=0.9, beta=0.9, diffusion_steps=10, embedding_scale=1)\n",
    "rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "import IPython.display as ipd\n",
    "display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-troopers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
